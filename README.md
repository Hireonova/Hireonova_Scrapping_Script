
# ğŸš€ Hireonova_Scrapping_Script

Hireonova Scraping Script is a powerful, AI-integrated Python-based job scraper that crawls and scrapes job listings from across the webâ€”including complex, JavaScript-heavy sites. It intelligently navigates through 200K+ pages using advanced heuristics and semantic tools to extract the most relevant job data such as title, company, location, skills, and apply URLs.

---

## ğŸ“Œ Features

- âœ… Scrapes static and dynamic (JavaScript-rendered) job sites using both `requests` and `Selenium`
- âœ… Dynamically identifies job cards using AI/NLP models (Ollama/Gemma-compatible)
- âœ… Performs BFS-based crawling to locate deep job listing pages
- âœ… Uses User-Agent rotation and IP-safe scraping with `fake-useragent`
- âœ… Automatically extracts structured job data (title, company, skills, etc.)
- âœ… Color-coded terminal output with `termcolor` for better log visibility
- âœ… Supports custom rules for known job boards like RemoteOK, Wellfound, etc.
- âœ… Built-in dark/light compatible frontend integration (React + Tailwind suggested)

---

## ğŸ§± Project Structure

```
Hireonova_Scrapping_Script/
â”‚
â”œâ”€â”€ main.py                  # Main entry point for scraping jobs
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ README.md                # Project documentation
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ nlp_extractor.py     # NLP-based job data extraction
â”‚   â””â”€â”€ bfs_crawler.py       # BFS-based URL traversal logic
â””â”€â”€ data/
    â””â”€â”€ job_results.json     # Saved job data (optional)
```

---

## ğŸ“¦ Requirements

Make sure you have **Python 3.8+** installed.

Install the required packages:

```bash
pip install -r requirements.txt
```

---

## âš™ï¸ Setup

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/Hireonova_Scrapping_Script.git
cd Hireonova_Scrapping_Script
```

### 2. (Optional) Create a Virtual Environment

```bash
python -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸš€ Running the Scraper

To start scraping, simply run:

```bash
python main.py
```

The scraper will:
- Use BFS to crawl and locate job pages
- Analyze job content with NLP
- Extract and print/save job data
- Output colored logs with job discovery status

---

## ğŸ›  Technologies Used

| Tool                  | Purpose                                 |
|-----------------------|-----------------------------------------|
| `requests`            | Lightweight static scraping             |
| `beautifulsoup4`      | HTML parsing and DOM traversal          |
| `selenium + undetected_chromedriver` | Dynamic JavaScript rendering |
| `pydantic`            | Schema validation for job entries       |
| `termcolor`           | Enhanced CLI logging                    |
| `fake-useragent`      | User-agent spoofing                     |
| `webdriver-manager`   | Auto manages compatible ChromeDrivers   |

---

## ğŸ”’ Anti-Bot Strategy

- Randomized User-Agent with `fake-useragent`
- Uses `undetected-chromedriver` to bypass bot detection
- Time delay + jitter added between requests
- BFS avoids recursive traps and duplicate paths

---

## ğŸ“¤ Output Format

Job listings are printed in terminal and can be saved as structured JSON:

```json
{
  "title": "Software Engineer - AI",
  "company": "TechNova Inc.",
  "location": "Remote",
  "skills": ["Python", "LLM", "NLP"],
  "description": "We are looking for...",
  "apply_url": "https://jobs.technova.com/apply/12345"
}
```

---

## ğŸ§  AI/NLP Integration (Optional but Recommended)

You can plug in **Ollama (Gemma 3B or similar)** to semantically identify job cards and extract key info:

- Run Ollama server locally
- Send HTML blocks to your endpoint like:
  ```python
  response = requests.post("http://localhost:11434/api/gemma", json={"prompt": html_chunk})
  ```

> This improves scraping accuracy especially on unstructured pages.

---

## ğŸ“… Future Enhancements

- [ ] Add resume-matching engine
- [ ] Add support for more job boards
- [ ] Use distributed crawling
- [ ] Connect to MongoDB/PostgreSQL for storage
- [ ] Add CLI arguments and filtering (e.g., role, location)

---

## ğŸ‘¨â€ğŸ’» Author

Made with â¤ï¸ by [Nickhil verma]  

---


